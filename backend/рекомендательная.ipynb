{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PchEeqm-eXbZ"
      },
      "source": [
        "# Hotel Recommendation System(ML Assignment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBOD3mb0fBF1",
        "outputId": "badb5c6d-e86b-4db0-ac26-09b0f386a4f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement keras.layers.core (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for keras.layers.core\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install keras.layers.core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiNnQd6ceXbd"
      },
      "source": [
        "# COD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pipreqs\n",
            "  Using cached pipreqs-0.5.0-py3-none-any.whl (33 kB)\n",
            "Collecting yarg==0.1.9\n",
            "  Using cached yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Collecting ipython==8.12.3\n",
            "  Using cached ipython-8.12.3-py3-none-any.whl (798 kB)\n",
            "Collecting nbconvert<8.0.0,>=7.11.0\n",
            "  Using cached nbconvert-7.16.4-py3-none-any.whl (257 kB)\n",
            "Collecting docopt==0.6.2\n",
            "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pickleshare\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: matplotlib-inline in /Users/liza/Library/Python/3.10/lib/python/site-packages (from ipython==8.12.3->pipreqs) (0.1.7)\n",
            "Requirement already satisfied: appnope in /Users/liza/Library/Python/3.10/lib/python/site-packages (from ipython==8.12.3->pipreqs) (0.1.4)\n",
            "Requirement already satisfied: traitlets>=5 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from ipython==8.12.3->pipreqs) (5.14.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from ipython==8.12.3->pipreqs) (4.9.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from ipython==8.12.3->pipreqs) (0.19.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from ipython==8.12.3->pipreqs) (3.0.43)\n",
            "Requirement already satisfied: decorator in /Users/liza/Library/Python/3.10/lib/python/site-packages (from ipython==8.12.3->pipreqs) (5.1.1)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from ipython==8.12.3->pipreqs) (2.18.0)\n",
            "Requirement already satisfied: stack-data in /Users/liza/Library/Python/3.10/lib/python/site-packages (from ipython==8.12.3->pipreqs) (0.6.3)\n",
            "Collecting backcall\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nbclient>=0.5.0\n",
            "  Downloading nbclient-0.10.0-py3-none-any.whl (25 kB)\n",
            "Collecting defusedxml\n",
            "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting tinycss2\n",
            "  Using cached tinycss2-1.3.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.7.2)\n",
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting markupsafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl (18 kB)\n",
            "Requirement already satisfied: packaging in /Users/liza/Library/Python/3.10/lib/python/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (24.0)\n",
            "Collecting nbformat>=5.7\n",
            "  Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
            "Collecting pandocfilters>=1.4.1\n",
            "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting jinja2>=3.0\n",
            "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Collecting bleach!=5.0.0\n",
            "  Downloading bleach-6.1.0-py3-none-any.whl (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.8/162.8 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mistune<4,>=2.0.3\n",
            "  Using cached mistune-3.0.2-py3-none-any.whl (47 kB)\n",
            "Collecting jupyterlab-pygments\n",
            "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from bleach!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.16.0)\n",
            "Collecting webencodings\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from jedi>=0.16->ipython==8.12.3->pipreqs) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from jupyter-core>=4.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.2.1)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (8.6.1)\n",
            "Collecting fastjsonschema>=2.15\n",
            "  Downloading fastjsonschema-2.19.1-py3-none-any.whl (23 kB)\n",
            "Collecting jsonschema>=2.6\n",
            "  Downloading jsonschema-4.22.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ptyprocess>=0.5 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from pexpect>4.3->ipython==8.12.3->pipreqs) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /Users/liza/Library/Python/3.10/lib/python/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.12.3->pipreqs) (0.2.13)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.4/120.4 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: executing>=1.2.0 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from stack-data->ipython==8.12.3->pipreqs) (2.0.1)\n",
            "Requirement already satisfied: pure-eval in /Users/liza/Library/Python/3.10/lib/python/site-packages (from stack-data->ipython==8.12.3->pipreqs) (0.2.2)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from stack-data->ipython==8.12.3->pipreqs) (2.4.1)\n",
            "Collecting jsonschema-specifications>=2023.03.6\n",
            "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Collecting rpds-py>=0.7.1\n",
            "  Downloading rpds_py-0.18.1-cp310-cp310-macosx_11_0_arm64.whl (322 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting referencing>=0.28.4\n",
            "  Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
            "Collecting attrs>=22.2.0\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado>=6.2 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.4)\n",
            "Requirement already satisfied: pyzmq>=23.0 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (26.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.9.0.post0)\n",
            "Using legacy 'setup.py install' for docopt, since package 'wheel' is not installed.\n",
            "Installing collected packages: webencodings, pickleshare, fastjsonschema, docopt, backcall, urllib3, tinycss2, soupsieve, rpds-py, pandocfilters, mistune, markupsafe, jupyterlab-pygments, idna, defusedxml, charset-normalizer, certifi, bleach, attrs, requests, referencing, jinja2, beautifulsoup4, yarg, jsonschema-specifications, ipython, jsonschema, nbformat, nbclient, nbconvert, pipreqs\n",
            "  Running setup.py install for docopt ... \u001b[?25ldone\n",
            "\u001b[?25h  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 8.24.0\n",
            "    Uninstalling ipython-8.24.0:\n",
            "      Successfully uninstalled ipython-8.24.0\n",
            "Successfully installed attrs-23.2.0 backcall-0.2.0 beautifulsoup4-4.12.3 bleach-6.1.0 certifi-2024.2.2 charset-normalizer-3.3.2 defusedxml-0.7.1 docopt-0.6.2 fastjsonschema-2.19.1 idna-3.7 ipython-8.12.3 jinja2-3.1.4 jsonschema-4.22.0 jsonschema-specifications-2023.12.1 jupyterlab-pygments-0.3.0 markupsafe-2.1.5 mistune-3.0.2 nbclient-0.10.0 nbconvert-7.16.4 nbformat-5.10.4 pandocfilters-1.5.1 pickleshare-0.7.5 pipreqs-0.5.0 referencing-0.35.1 requests-2.31.0 rpds-py-0.18.1 soupsieve-2.5 tinycss2-1.3.0 urllib3-2.2.1 webencodings-0.5.1 yarg-0.1.9\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pipreqs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.26.4)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.2.2-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
            "Collecting tzdata>=2022.7\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 KB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.8.4-cp310-cp310-macosx_11_0_arm64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.51.0-cp310-cp310-macosx_10_9_universal2.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
            "  Downloading kiwisolver-1.4.5-cp310-cp310-macosx_11_0_arm64.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.2.1-cp310-cp310-macosx_11_0_arm64.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 KB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
            "Collecting pillow>=8\n",
            "  Downloading pillow-10.3.0-cp310-cp310-macosx_11_0_arm64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing>=2.3.1\n",
            "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 kiwisolver-1.4.5 matplotlib-3.8.4 pillow-10.3.0 pyparsing-3.1.2\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_690hlorvpy/croot/aiohttp_1670009554132/work\n",
            "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_690hlorvpy/croot/aiohttp_1670009554132/work'\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -r /Users/liza/Downloads/hotel-hotel-latest/backend/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting optree\n",
            "  Downloading optree-0.11.0-cp310-cp310-macosx_11_0_arm64.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.3/273.3 KB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keras) (1.26.4)\n",
            "Collecting absl-py\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes\n",
            "  Downloading ml_dtypes-0.4.0-cp310-cp310-macosx_10_9_universal2.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.9/390.9 KB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich\n",
            "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 KB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-3.11.0-cp310-cp310-macosx_11_0_arm64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from optree->keras) (4.11.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from rich->keras) (2.18.0)\n",
            "Collecting markdown-it-py>=2.2.0\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: namex, optree, ml-dtypes, mdurl, h5py, absl-py, markdown-it-py, rich, keras\n",
            "Successfully installed absl-py-2.1.0 h5py-3.11.0 keras-3.3.3 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.0 namex-0.0.8 optree-0.11.0 rich-13.7.1\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-macosx_12_0_arm64.whl (227.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.0/227.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from tensorflow) (4.11.0)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard<2.17,>=2.16\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting libclang>=13.0.0\n",
            "  Downloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
            "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
            "Collecting flatbuffers>=23.5.26\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: keras>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow) (3.3.3)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow) (58.1.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from tensorflow) (1.16.0)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.63.0-cp310-cp310-macosx_12_0_universal2.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes~=0.3.1\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-macosx_10_9_universal2.whl (389 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /Users/liza/Library/Python/3.10/lib/python/site-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-macosx_12_0_arm64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting wheel<1.0,>=0.23.0\n",
            "  Downloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 KB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/liza/Library/Python/3.10/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Installing collected packages: libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, ml-dtypes, markdown, grpcio, google-pasta, gast, tensorboard, astunparse, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.63.0 libclang-18.1.1 markdown-3.6 ml-dtypes-0.3.2 opt-einsum-3.3.0 protobuf-4.25.3 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 werkzeug-3.0.3 wheel-0.43.0 wrapt-1.16.0\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
            "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
            "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
            "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
            "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m More information is available at\n",
            "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-macosx_12_0_arm64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.13.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.4.2-cp310-cp310-macosx_12_0_arm64.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.0-cp310-cp310-macosx_12_0_arm64.whl (30.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
            "Successfully installed scikit-learn-1.4.2 scipy-1.13.0 threadpoolctl-3.5.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex>=2021.8.3\n",
            "  Downloading regex-2024.5.10-cp310-cp310-macosx_11_0_arm64.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\n",
            "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.8.1 regex-2024.5.10 tqdm-4.66.4\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (22.0.4)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.4\n",
            "    Uninstalling pip-22.0.4:\n",
            "      Successfully uninstalled pip-22.0.4\n",
            "Successfully installed pip-24.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement math (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for math\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-2.0.3-py3-none-macosx_12_0_arm64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from xgboost) (1.13.0)\n",
            "Downloading xgboost-2.0.3-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost\n",
            "Successfully installed xgboost-2.0.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-surprise) (1.13.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-macosx_10_9_universal2.whl size=1592425 sha256=49e5a58a4ca367cab263a5aed9fdcd910b4ed00b772320a1033ad9d845d06722\n",
            "  Stored in directory: /Users/liza/Library/Caches/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openpyxl\n",
            "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting et-xmlfile (from openpyxl)\n",
            "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2aFiJdX2eXbd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from keras.models import Sequential\n",
        "# from keras.layers.core import Dense,Activation,Dropout\n",
        "# from keras.layers import LSTM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from keras.callbacks import EarlyStopping\n",
        "import math\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from scipy.sparse import csr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5T03-KK0eXbf"
      },
      "outputs": [],
      "source": [
        "hotel_details=pd.read_excel('/Users/liza/Downloads/hotel-hotel-latest/backend/hotels.xlsx')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wxi6cXgnfVQ-"
      },
      "outputs": [],
      "source": [
        "hotel_rooms=pd.read_excel('/Users/liza/Downloads/hotel-hotel-latest/backend/room.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UeVDsTkQoQYK"
      },
      "outputs": [],
      "source": [
        "events_df = pd.read_excel('/Users/liza/Downloads/hotel-hotel-latest/backend/orders.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2FmaldXeXbf"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qh3gJCcteXbg"
      },
      "outputs": [],
      "source": [
        "hotel=pd.merge(hotel_rooms,hotel_details,left_on='hotel_id',right_on='id',how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dGoOvIVJ0lFD"
      },
      "outputs": [],
      "source": [
        "\n",
        "hotel = hotel.merge(events_df, on='hotel_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "hotel = hotel.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JqwcVsXTeXbg"
      },
      "outputs": [],
      "source": [
        "del hotel['id_x']\n",
        "del hotel['id_y']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "fvug414KeXbi"
      },
      "outputs": [],
      "source": [
        "hotel['description']=hotel['description'].str.replace(': ;',',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Series.unique of 0             31\n",
              "1             31\n",
              "2             31\n",
              "3             31\n",
              "4             31\n",
              "            ... \n",
              "16792874    1534\n",
              "16792875    1534\n",
              "16792876    1534\n",
              "16792877    1534\n",
              "16792878    1534\n",
              "Name: hotel_id, Length: 16792244, dtype: int64>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hotel.hotel_id.unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C73nYvIjeXbh"
      },
      "source": [
        "# Рекомендательная система, основанная только на городе и рейтингах отеля"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3qjvzKIkeXbi"
      },
      "outputs": [],
      "source": [
        "def requirementbased(country, number, features):\n",
        "    # Чтение данных из Excel файлов с информацией о гостиницах, номерах и заказах\n",
        "    hotel_details = pd.read_excel('/Users/liza/Downloads/hotel-hotel-latest/backend/hotels.xlsx')\n",
        "    hotel_rooms = pd.read_excel('/Users/liza/Downloads/hotel-hotel-latest/backend/room.xlsx')\n",
        "    events_df = pd.read_excel('/Users/liza/Downloads/hotel-hotel-latest/backend/orders.xlsx')\n",
        "    \n",
        "    # Объединение таблиц по идентификатору гостиницы\n",
        "    hotel = pd.merge(hotel_rooms, hotel_details, left_on='hotel_id', right_on='id', how='inner')\n",
        "    hotel = hotel.merge(events_df, on='hotel_id', how='left')\n",
        "    \n",
        "    # Удаление строк с отсутствующими значениями\n",
        "    hotel = hotel.dropna()\n",
        "    \n",
        "    # Удаление лишних столбцов\n",
        "    del hotel['id_x']\n",
        "    del hotel['id_y']\n",
        "    \n",
        "    # Преобразование текстовых данных в нижний регистр\n",
        "    hotel['description'] = hotel['description'].str.replace(': ;', ',')\n",
        "    hotel['country'] = hotel['country'].str.lower()\n",
        "    hotel['description'] = hotel['description'].str.lower()\n",
        "    \n",
        "    # Преобразование входных особенностей в нижний регистр и токенизация\n",
        "    features = features.lower()\n",
        "    features_tokens = word_tokenize(features)\n",
        "    \n",
        "    # Удаление стоп-слов и лемматизация особенностей\n",
        "    '''1. sw = stopwords.words('english'): Здесь загружаются стоп-слова на английском языке из библиотеки NLTK. Стоп-слова — это слова, которые обычно не несут смысловой нагрузки и могут быть исключены из анализа текста.\n",
        "\n",
        "2. lemm = WordNetLemmatizer(): Создается объект WordNetLemmatizer, который используется для лемматизации слов. Лемматизация — это процесс приведения слова к его базовой форме (лемме).\n",
        "\n",
        "3. f1_set = {w for w in features_tokens if not w in sw}: Здесь создается множество f1_set, содержащее слова из features_tokens, которые не являются стоп-словами. features_tokens предположительно содержит токены (слова) текста для анализа.\n",
        "\n",
        "4. f_set = set(): Создается пустое множество f_set, которое будет содержать лемматизированные версии слов из f1_set.\n",
        "\n",
        "5. for se in f1_set:: Начинается цикл по элементам множества f1_set.\n",
        "\n",
        "6. f_set.add(lemm.lemmatize(se)): Для каждого элемента se из f1_set происходит лемматизация с помощью lemm.lemmatize(se), после чего лемматизированное слово добавляется в множество f_set.\n",
        "\n",
        "Таким образом, в результате выполнения этого кода мы получаем множество f_set, содержащее лемматизированные версии слов из features_tokens, исключая стоп-слова, которые представлены в переменной sw. Лемматизация помогает сократить разнообразие форм слова до их базовых форм для более точного анализа текста.'''\n",
        "    sw = stopwords.words('english')\n",
        "    lemm = WordNetLemmatizer()\n",
        "    f1_set = {w for w in features_tokens if not w in sw}\n",
        "    f_set = set()\n",
        "    for se in f1_set:\n",
        "        f_set.add(lemm.lemmatize(se))\n",
        "    \n",
        "    # Фильтрация данных по стране и максимальному количеству номеров\n",
        "    reqbased = hotel[hotel['country'] == country.lower()]\n",
        "    reqbased = reqbased[reqbased['max_count'] == number]\n",
        "    reqbased = reqbased.set_index(np.arange(reqbased.shape[0]))\n",
        "    \n",
        "    l1 = []; l2 = []; cos = []\n",
        "    \n",
        "    # Вычисление сходства особенностей для каждой гостиницы\n",
        "    '''Данная часть кода представляет собой цикл, который проходит по каждой строке данных в DataFrame reqbased и вычисляет показатель сходства между особенностями гостиницы (описанными в столбце 'description') и особенностями, указанными пользователем в переменной features.\n",
        "\n",
        "1. temp_tokens = word_tokenize(reqbased['description'][i]): Для каждой строки в reqbased извлекаются текстовые данные из столбца 'description' и токенизируются, то есть разбиваются на отдельные слова.\n",
        "\n",
        "2. temp1_set = {w for w in temp_tokens if not w in sw}: Создается множество temp1_set, содержащее уникальные слова из описания гостиницы, исключая стоп-слова (слова, которые обычно не несут смысловой нагрузки).\n",
        "\n",
        "3. temp_set = set(): Создается пустое множество temp_set, которое будет содержать лемматизированные слова из описания гостиницы.\n",
        "\n",
        "4. for se in temp1_set: temp_set.add(lemm.lemmatize(se)): Каждое слово из temp1_set лемматизируется (приводится к начальной форме) с использованием WordNetLemmatizer и добавляется в temp_set.\n",
        "\n",
        "5. rvector = temp_set.intersection(f_set): Создается множество rvector, содержащее пересечение слов из описания гостиницы (temp_set) и особенностей, указанных пользователем (f_set).\n",
        "\n",
        "6. cos.append(len(rvector)): Длина множества rvector (то есть количество слов, которые пересекаются) добавляется в список cos, который будет использоваться для оценки сходства между описанием гостиницы и требованиями пользователя.\n",
        "\n",
        "Этот процесс повторяется для каждой строки данных в reqbased, чтобы вычислить показатель сходства особенностей для каждой гостиницы.'''\n",
        "\n",
        "    for i in range(reqbased.shape[0]):\n",
        "        temp_tokens = word_tokenize(reqbased['description'][i])\n",
        "        temp1_set = {w for w in temp_tokens if not w in sw}\n",
        "        temp_set = set()\n",
        "        for se in temp1_set:\n",
        "            temp_set.add(lemm.lemmatize(se))\n",
        "        rvector = temp_set.intersection(f_set)\n",
        "        cos.append(len(rvector))\n",
        "    \n",
        "    # Добавление столбца с показателем сходства к данным и сортировка по этому показателю\n",
        "    reqbased['similarity'] = cos\n",
        "    reqbased = reqbased.sort_values(by='similarity', ascending=False)\n",
        "    \n",
        "    # Удаление дубликатов гостиниц и возврат топ-10 результатов\n",
        "    reqbased.drop_duplicates(subset='hotel_id', keep='first', inplace=True)\n",
        "    return reqbased[['name', 'country_rus', 'stars', 'price', 'hotel_img', 'room_img', 'description', 'category']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mwkw8d5WuaUo"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "Pkl_Filename = \"Pickle_RL_Model.pkl\"\n",
        "with open(Pkl_Filename, 'wb') as file:\n",
        "    pickle.dump(requirementbased, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/liza/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /Users/liza/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/liza/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PZkM1b0PeXbi"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>country_rus</th>\n",
              "      <th>stars</th>\n",
              "      <th>price</th>\n",
              "      <th>hotel_img</th>\n",
              "      <th>room_img</th>\n",
              "      <th>description</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hotel Singapore</td>\n",
              "      <td>Италия</td>\n",
              "      <td>3</td>\n",
              "      <td>23602</td>\n",
              "      <td>https://www.tourdom.ru/upload/iblock/7fa/7fab8...</td>\n",
              "      <td>https://static.tildacdn.com/tild3835-3863-4339...</td>\n",
              "      <td>air conditioning,alarm clock,bathrobes,carpeti...</td>\n",
              "      <td>Studio King Suite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1635</th>\n",
              "      <td>Schlossberg</td>\n",
              "      <td>Италия</td>\n",
              "      <td>2</td>\n",
              "      <td>10546</td>\n",
              "      <td>https://guide-tours.ru/wp-content/uploads/2022...</td>\n",
              "      <td>https://dolphinhotel.ru/_cache/udb1-rec13-fiel...</td>\n",
              "      <td>air conditioning,daily housekeeping,desk,free ...</td>\n",
              "      <td>Standard Single Room</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>Hotel Holiday</td>\n",
              "      <td>Италия</td>\n",
              "      <td>3</td>\n",
              "      <td>19824</td>\n",
              "      <td>https://otdih.nakubani.ru/m/catalogBig/65d384f...</td>\n",
              "      <td>https://standarthotel.com/upload/iblock/1ea/1e...</td>\n",
              "      <td>air conditioning,free wi-fi in all rooms!,hair...</td>\n",
              "      <td>Single Standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>Hotel Vedig</td>\n",
              "      <td>Италия</td>\n",
              "      <td>4</td>\n",
              "      <td>21148</td>\n",
              "      <td>https://tophotels.ru/icache/hotel_photos/1/880...</td>\n",
              "      <td>https://uniqhotel.ru/wp-content/uploads/intery...</td>\n",
              "      <td>air conditioning,carpeting,closet,desk,free wi...</td>\n",
              "      <td>Single Room</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 name country_rus  stars  price  \\\n",
              "0     Hotel Singapore      Италия      3  23602   \n",
              "1635      Schlossberg      Италия      2  10546   \n",
              "426     Hotel Holiday      Италия      3  19824   \n",
              "1029      Hotel Vedig      Италия      4  21148   \n",
              "\n",
              "                                              hotel_img  \\\n",
              "0     https://www.tourdom.ru/upload/iblock/7fa/7fab8...   \n",
              "1635  https://guide-tours.ru/wp-content/uploads/2022...   \n",
              "426   https://otdih.nakubani.ru/m/catalogBig/65d384f...   \n",
              "1029  https://tophotels.ru/icache/hotel_photos/1/880...   \n",
              "\n",
              "                                               room_img  \\\n",
              "0     https://static.tildacdn.com/tild3835-3863-4339...   \n",
              "1635  https://dolphinhotel.ru/_cache/udb1-rec13-fiel...   \n",
              "426   https://standarthotel.com/upload/iblock/1ea/1e...   \n",
              "1029  https://uniqhotel.ru/wp-content/uploads/intery...   \n",
              "\n",
              "                                            description              category  \n",
              "0     air conditioning,alarm clock,bathrobes,carpeti...     Studio King Suite  \n",
              "1635  air conditioning,daily housekeeping,desk,free ...  Standard Single Room  \n",
              "426   air conditioning,free wi-fi in all rooms!,hair...       Single Standard  \n",
              "1029  air conditioning,carpeting,closet,desk,free wi...           Single Room  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "requirementbased('Italy',1,'I need a extra toilet and room should be completely air conditioned.I should have a bathrobe.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Рекомендательная система по ЦА пользователя"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "d0Pu26fj9xyN"
      },
      "outputs": [],
      "source": [
        "hotel['interaction_value'] = hotel['status_name'].apply(lambda x: 0.6 if x == 'order' else (0.8 if x == 'pay' else 0.4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "item_index_map = {item: idx for idx, item in enumerate(hotel['hotel_id'].unique())}\n",
        "visitor_index_map = {visitor: idx for idx, visitor in enumerate(hotel['user_id'].unique())}\n",
        "cluster_index_map = {cluster: idx for idx, cluster in enumerate(hotel['cluster'].unique())}\n",
        "\n",
        "row_indices = [visitor_index_map[visitor] for visitor in hotel['user_id']]\n",
        "col_indices = [item_index_map[item] for item in hotel['hotel_id']]\n",
        "interaction_values = hotel['interaction_value']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "interaction_matrix = csr_matrix((interaction_values, (row_indices, col_indices)),\n",
        "                                 shape=(len(visitor_index_map), len(item_index_map)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "RMSE (testset)    0.0997  0.0997  0.0997  0.0997  0.1000  0.0998  0.0001  \n",
            "MAE (testset)     0.0994  0.0995  0.0995  0.0995  0.0996  0.0995  0.0001  \n",
            "Fit time          113.57  113.84  116.88  123.75  115.51  116.71  3.72    \n",
            "Test time         48.27   53.95   32.94   44.42   33.34   42.59   8.29    \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x28ed8efb0>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from surprise import SVD\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "reader = Reader(rating_scale=(0, 1))  \n",
        "data = Dataset.load_from_df(hotel[['user_id', 'hotel_id', 'interaction_value']], reader)\n",
        "\n",
        "model = SVD()\n",
        "\n",
        "cross_validate(model, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
        "\n",
        "trainset = data.build_full_trainset()\n",
        "model.fit(trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_n_recommendations(model, user_id, num_items, n=10):\n",
        "    recommendations = []\n",
        "    for item_id in range(1, num_items + 1):\n",
        "        prediction = model.predict(user_id, item_id)\n",
        "        recommendations.append((item_id, prediction.est))\n",
        "\n",
        "    # Sort recommendations by estimated rating\n",
        "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    top_n_recommendations = recommendations[:n]\n",
        "    return top_n_recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from surprise.dump import dump\n",
        "\n",
        "model_filename = 'trained_model.pkl'\n",
        "dump(model_filename, algo=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "from surprise.dump import load\n",
        "\n",
        "loaded_model = load('trained_model.pkl')[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Лучшие рекомендации для пользователей 936143.0\n",
            "Item: 1 Предполагаемый рейтинг по продукту: 0.7066467026053401\n",
            "Item: 2 Предполагаемый рейтинг по продукту: 0.7066467026053401\n",
            "Item: 3 Предполагаемый рейтинг по продукту: 0.7066467026053401\n",
            "Item: 4 Предполагаемый рейтинг по продукту: 0.7066467026053401\n",
            "Item: 5 Предполагаемый рейтинг по продукту: 0.7066467026053401\n",
            "Item: 6 Предполагаемый рейтинг по продукту: 0.7066467026053401\n",
            "Item: 7 Предполагаемый рейтинг по продукту: 0.7066467026053401\n",
            "Item: 8 Предполагаемый рейтинг по продукту: 0.7066467026053401\n",
            "Item: 9 Предполагаемый рейтинг по продукту: 0.7066467026053401\n",
            "Item: 10 Предполагаемый рейтинг по продукту: 0.7066467026053401\n"
          ]
        }
      ],
      "source": [
        "user_id = hotel['user_id'].sample().values[0]\n",
        "\n",
        "num_items = hotel['hotel_id'].nunique()\n",
        "top_recommendations = get_top_n_recommendations(loaded_model, user_id, num_items, n=10)\n",
        "print(\"Лучшие рекомендации для пользователей\", user_id)\n",
        "for item_id, estimated_rating in top_recommendations:\n",
        "    print(\"Item:\", item_id, \"Предполагаемый рейтинг по продукту:\", estimated_rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 0.7099\n",
            "[(31, 1), (97, 1), (138, 1), (150, 1), (287, 1), (378, 1), (515, 1), (597, 1), (652, 1), (654, 1)]\n"
          ]
        }
      ],
      "source": [
        "from surprise import Dataset, Reader\n",
        "from surprise import SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "# Подготовка данных\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "data = Dataset.load_from_df(hotel[['user_id', 'hotel_id', 'status']], reader)\n",
        "\n",
        "\n",
        "# Обучение модели\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "model = SVD()\n",
        "model.fit(trainset)\n",
        "\n",
        "# Оценка модели\n",
        "predictions = model.test(testset)\n",
        "accuracy.rmse(predictions)\n",
        "\n",
        "# Предсказание рекомендаций для конкретного пользователя\n",
        "user_id = int(hotel['user_id'].sample().values[0])\n",
        "top_n = 10\n",
        "recommendations = {}\n",
        "for hotel_id in hotel['hotel_id'].unique():\n",
        "    pred = model.predict(user_id, hotel_id)\n",
        "    recommendations[hotel_id] = pred.est\n",
        "\n",
        "top_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "print(top_recommendations)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Рекомендательная система коллаборативный метод 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import vstack\n",
        "from scipy import sparse\n",
        "from scipy.sparse.linalg import spsolve\n",
        "from subprocess import check_output\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from lightfm import LightFM\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import vstack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recommendation lightFM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_activity_count = dict()\n",
        "for row in hotel.itertuples():\n",
        "    if row.user_id not in user_activity_count:\n",
        "        user_activity_count[row.user_id] = {'order': 0, 'pay': 0}\n",
        "    if row.status_name == 'order':\n",
        "        user_activity_count[row.user_id]['order'] += 1 \n",
        "    elif row.status_name == 'pay':\n",
        "        user_activity_count[row.user_id]['pay'] += 1\n",
        "\n",
        "d = pd.DataFrame(user_activity_count)\n",
        "dataframe = d.transpose()\n",
        "# Диапазон активности\n",
        "dataframe['activity'] = dataframe['order'] + dataframe['pay'] \n",
        "# Удаление пользователей с одним просмотром\n",
        "cleaned_data = dataframe[dataframe['activity'] != 1]\n",
        "# all_users содержит идентификаторы пользователей с более чем 1 активностью в отеле (400000)\n",
        "all_users = set(cleaned_data.index.values)\n",
        "all_items = set(hotel['hotel_id'])\n",
        "# todo: необходимо очистить элементы, которые были просмотрены только один раз\n",
        "\n",
        "user_id_to_index_mapping = {}\n",
        "hotel_id_to_index_mapping = {}\n",
        "vid = 0\n",
        "iid = 0\n",
        "for row in hotel.itertuples():\n",
        "    if row.user_id in all_users and row.user_id not in user_id_to_index_mapping:\n",
        "        user_id_to_index_mapping[row.user_id] = vid\n",
        "        vid = vid + 1\n",
        "\n",
        "    if row.hotel_id in all_items and row.hotel_id not in hotel_id_to_index_mapping:\n",
        "        hotel_id_to_index_mapping[row.hotel_id] = iid\n",
        "        iid = iid + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_items = hotel[hotel.hotel_id.isin(all_items)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_items = filtered_items.drop_duplicates(['hotel_id','cluster'])\n",
        "filtered_items.sort_values(by='hotel_id', inplace=True)\n",
        "item_to_property_matrix = filtered_items[['hotel_id','cluster']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "useful_cols = list()\n",
        "cols = item_to_property_matrix.columns\n",
        "for col in cols:\n",
        "    value = len(item_to_property_matrix[col].value_counts())\n",
        "    if value < 50:\n",
        "        useful_cols.insert(0, col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "item_to_property_matrix = item_to_property_matrix[useful_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "item_to_property_matrix_one_hot_sparse = pd.get_dummies(item_to_property_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "item_to_property_matrix_sparse = csr_matrix(item_to_property_matrix_one_hot_sparse.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [2. 0. 0. ... 0. 0. 0.]\n",
            " [2. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 2.]\n",
            " [0. 0. 0. ... 0. 0. 2.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Создаем нулевую матрицу взаимодействий с размерами (количество пользователей, количество отелей)\n",
        "num_users = len(user_id_to_index_mapping)\n",
        "num_hotels = len(hotel_id_to_index_mapping)\n",
        "interactions_matrix = np.zeros((num_users, num_hotels))\n",
        "\n",
        "# Заполняем матрицу взаимодействий на основе данных из DataFrame hotel\n",
        "for index, row in hotel.iterrows():\n",
        "    user_id = row['user_id']\n",
        "    hotel_id = row['hotel_id']\n",
        "    interaction_type = row['status']  \n",
        "\n",
        "    user_index = user_id_to_index_mapping[user_id]\n",
        "    hotel_index = hotel_id_to_index_mapping[hotel_id]\n",
        "\n",
        "    interactions_matrix[user_index, hotel_index] = interaction_type\n",
        "\n",
        "# Выводим полученную матрицу взаимодействий\n",
        "print(interactions_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:00<00:00, 1841.30it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<lightfm.lightfm.LightFM at 0x5198f0940>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import scipy.sparse as sp\n",
        "\n",
        "interactions_matrix_sparse = sp.coo_matrix(interactions_matrix)\n",
        "item_to_property_matrix_sparse = sp.coo_matrix(item_to_property_matrix_sparse)\n",
        "no_comp, lr, ep = 30, 0.01, 10 \n",
        "\n",
        "model = LightFM(no_components=no_comp, learning_rate=lr, loss='warp')\n",
        "model.fit(\n",
        "    interactions_matrix_sparse,\n",
        "    item_features=item_to_property_matrix_sparse,\n",
        "    epochs=ep,\n",
        "    num_threads=4,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top recommended hotels for user: [0 1 2 3 4]\n"
          ]
        }
      ],
      "source": [
        "# Функция предсказания для пользователя\n",
        "def predict_top_hotels_for_user(user_id, num_items_to_predict):\n",
        "    scores = model.predict(user_id, np.arange(interactions_matrix_sparse.shape[1]), item_features=item_to_property_matrix_sparse)\n",
        "    top_items = np.argsort(-scores)[:num_items_to_predict]\n",
        "    return top_items\n",
        "\n",
        "# Пример использования функции предсказания\n",
        "user_id = 5\n",
        "num_items_to_predict = 5\n",
        "top_hotels = predict_top_hotels_for_user(user_id, num_items_to_predict)\n",
        "print(\"Top recommended hotels for user:\", top_hotels)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
